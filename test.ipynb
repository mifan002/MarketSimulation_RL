{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-05-15T20:24:44.436306Z",
     "end_time": "2023-05-15T20:25:17.053637Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 10.9     |\n",
      "|    ep_rew_mean     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 862      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 11.2         |\n",
      "|    ep_rew_mean          | 1.31e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 743          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028625051 |\n",
      "|    clip_fraction        | 0.00327      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -9.23        |\n",
      "|    explained_variance   | -5.83e-05    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.89e+05     |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0169      |\n",
      "|    value_loss           | 1e+06        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 12.7         |\n",
      "|    ep_rew_mean          | 1.48e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 702          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 8            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017151479 |\n",
      "|    clip_fraction        | 0.000293     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -9.23        |\n",
      "|    explained_variance   | 0.000105     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.91e+05     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.0121      |\n",
      "|    value_loss           | 1.19e+06     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 10.3         |\n",
      "|    ep_rew_mean          | 1.27e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 692          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012229136 |\n",
      "|    clip_fraction        | 4.88e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -9.22        |\n",
      "|    explained_variance   | 2.28e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.14e+05     |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00964     |\n",
      "|    value_loss           | 1.41e+06     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 14           |\n",
      "|    ep_rew_mean          | 1.67e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 684          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017279962 |\n",
      "|    clip_fraction        | 0.000635     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -9.22        |\n",
      "|    explained_variance   | 7.81e-06     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.84e+05     |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0124      |\n",
      "|    value_loss           | 1.23e+06     |\n",
      "------------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.92     |\n",
      "|    ep_rew_mean        | 990      |\n",
      "| time/                 |          |\n",
      "|    fps                | 605      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -8.95    |\n",
      "|    explained_variance | -0.00422 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 1.97e+03 |\n",
      "|    value_loss         | 6.94e+04 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.51     |\n",
      "|    ep_rew_mean        | 1.17e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 605      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -8.72    |\n",
      "|    explained_variance | 2e-05    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 1.39e+03 |\n",
      "|    value_loss         | 2.98e+04 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 10.1      |\n",
      "|    ep_rew_mean        | 1.25e+03  |\n",
      "| time/                 |           |\n",
      "|    fps                | 604       |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -8.85     |\n",
      "|    explained_variance | -0.000445 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | 2.89e+03  |\n",
      "|    value_loss         | 1.18e+05  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 12.4     |\n",
      "|    ep_rew_mean        | 1.49e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 604      |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -8.49    |\n",
      "|    explained_variance | -1.2e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 3.25e+03 |\n",
      "|    value_loss         | 1.7e+05  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 14.7     |\n",
      "|    ep_rew_mean        | 1.73e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 597      |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -8.46    |\n",
      "|    explained_variance | 1.13e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 2.45e+03 |\n",
      "|    value_loss         | 1.15e+05 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 19.3      |\n",
      "|    ep_rew_mean        | 2.22e+03  |\n",
      "| time/                 |           |\n",
      "|    fps                | 593       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -8.36     |\n",
      "|    explained_variance | -8.58e-06 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | 3.85e+03  |\n",
      "|    value_loss         | 2.18e+05  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 23       |\n",
      "|    ep_rew_mean        | 2.67e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 593      |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.96    |\n",
      "|    explained_variance | 1.55e-06 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 2.57e+03 |\n",
      "|    value_loss         | 1.13e+05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 28.1     |\n",
      "|    ep_rew_mean        | 3.26e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 593      |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.83    |\n",
      "|    explained_variance | 7.99e-06 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 2.08e+03 |\n",
      "|    value_loss         | 7.58e+04 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 30.5      |\n",
      "|    ep_rew_mean        | 3.57e+03  |\n",
      "| time/                 |           |\n",
      "|    fps                | 594       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.11     |\n",
      "|    explained_variance | -8.34e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 2.73e+03  |\n",
      "|    value_loss         | 2.58e+05  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 33.4     |\n",
      "|    ep_rew_mean        | 3.95e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 596      |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 8        |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.17    |\n",
      "|    explained_variance | -2.5e-06 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 2.57e+03 |\n",
      "|    value_loss         | 2.12e+05 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 33.4      |\n",
      "|    ep_rew_mean        | 3.95e+03  |\n",
      "| time/                 |           |\n",
      "|    fps                | 598       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 9         |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.98     |\n",
      "|    explained_variance | -4.77e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | 1.65e+03  |\n",
      "|    value_loss         | 2.48e+05  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 43.3     |\n",
      "|    ep_rew_mean        | 5.27e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 598      |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 10       |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -6.02    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 2.88e+03 |\n",
      "|    value_loss         | 3.39e+05 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 43.3      |\n",
      "|    ep_rew_mean        | 5.27e+03  |\n",
      "| time/                 |           |\n",
      "|    fps                | 599       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.99     |\n",
      "|    explained_variance | -7.15e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 2.75e+03  |\n",
      "|    value_loss         | 2.6e+05   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 43.3     |\n",
      "|    ep_rew_mean        | 5.27e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 598      |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 11       |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -5.4     |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 3.34e+03 |\n",
      "|    value_loss         | 2.08e+05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 43.3     |\n",
      "|    ep_rew_mean        | 5.27e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 599      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 12       |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.78    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 1.56e+03 |\n",
      "|    value_loss         | 2.31e+05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 43.3     |\n",
      "|    ep_rew_mean        | 5.27e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 599      |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 13       |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.41    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 1.64e+03 |\n",
      "|    value_loss         | 3.05e+05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 43.3     |\n",
      "|    ep_rew_mean        | 5.27e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 599      |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 14       |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.65    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 1.63e+03 |\n",
      "|    value_loss         | 3.16e+05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 43.3     |\n",
      "|    ep_rew_mean        | 5.27e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 598      |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 15       |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.33    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 2.23e+03 |\n",
      "|    value_loss         | 3.27e+05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 43.3     |\n",
      "|    ep_rew_mean        | 5.27e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 599      |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 15       |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.22    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 1.18e+03 |\n",
      "|    value_loss         | 3.1e+05  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 43.3     |\n",
      "|    ep_rew_mean        | 5.27e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 598      |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 16       |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.49    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 1.41e+03 |\n",
      "|    value_loss         | 3.32e+05 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": "<stable_baselines3.a2c.a2c.A2C at 0x7fe230034670>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the environment\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO, A2C\n",
    "\n",
    "# Define the market environment\n",
    "class MarketEnvironment(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(MarketEnvironment, self).__init__()\n",
    "\n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects\n",
    "        # Example when using discrete actions, Box(2,) for two sellers\n",
    "        self.action_space = spaces.MultiDiscrete([101, 101])\n",
    "\n",
    "        # Prices could range from 0 to 100, there are four buyers\n",
    "        self.observation_space = spaces.Box(low=0, high=100, shape=(6,))\n",
    "\n",
    "\n",
    "        # Initialize state\n",
    "        self.reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        # Execute one time step within the environment\n",
    "        assert self.action_space.contains(action)\n",
    "\n",
    "        # Simple model: buyers buy from the cheapest seller\n",
    "        sorted_sellers = np.argsort(action)\n",
    "        self.state[0] = action[sorted_sellers[0]]\n",
    "        self.state[1] = action[sorted_sellers[1]]\n",
    "\n",
    "        # Distribute the buyers\n",
    "        for i in range(2, 6):\n",
    "            if self.state[i] >= self.state[0]:\n",
    "                self.state[0] += self.state[i]\n",
    "                self.state[i] = 0\n",
    "            elif self.state[i] >= self.state[1]:\n",
    "                self.state[1] += self.state[i]\n",
    "                self.state[i] = 0\n",
    "\n",
    "        # Set reward as the profit of the sellers\n",
    "        reward = self.state[0] + self.state[1]\n",
    "\n",
    "        # Set done flag if all buyers have bought the products\n",
    "        done = np.sum(self.state[2:]) == 0\n",
    "\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the state of the environment to an initial state\n",
    "        self.state = np.zeros(6)\n",
    "        # Initialize buyers' willingness to pay\n",
    "        self.state[2:6] = np.random.uniform(low=0, high=100, size=4)\n",
    "        return self.state\n",
    "\n",
    "# Initialize environment\n",
    "env = MarketEnvironment()\n",
    "\n",
    "# Initialize reinforcement learning agents\n",
    "model1 = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model2 = A2C(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# Train agents\n",
    "model1.learn(total_timesteps=10000)\n",
    "model2.learn(total_timesteps=10000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward for the first agent: 1502.17164953282\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 19\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMean reward for the first agent: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmean_reward1\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m# Evaluate the second agent\u001B[39;00m\n\u001B[0;32m---> 19\u001B[0m mean_reward2 \u001B[38;5;241m=\u001B[39m \u001B[43mevaluate_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menv\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMean reward for the second agent: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmean_reward2\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[9], line 8\u001B[0m, in \u001B[0;36mevaluate_model\u001B[0;34m(model, env, num_episodes)\u001B[0m\n\u001B[1;32m      6\u001B[0m episode_reward \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m done:\n\u001B[0;32m----> 8\u001B[0m     action, _ \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m     obs, reward, done, info \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(action)\n\u001B[1;32m     10\u001B[0m     episode_reward \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m reward\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/stable_baselines3/common/base_class.py:539\u001B[0m, in \u001B[0;36mBaseAlgorithm.predict\u001B[0;34m(self, observation, state, episode_start, deterministic)\u001B[0m\n\u001B[1;32m    519\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict\u001B[39m(\n\u001B[1;32m    520\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    521\u001B[0m     observation: Union[np\u001B[38;5;241m.\u001B[39mndarray, Dict[\u001B[38;5;28mstr\u001B[39m, np\u001B[38;5;241m.\u001B[39mndarray]],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    524\u001B[0m     deterministic: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    525\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[np\u001B[38;5;241m.\u001B[39mndarray, Optional[Tuple[np\u001B[38;5;241m.\u001B[39mndarray, \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m]]]:\n\u001B[1;32m    526\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    527\u001B[0m \u001B[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001B[39;00m\n\u001B[1;32m    528\u001B[0m \u001B[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    537\u001B[0m \u001B[38;5;124;03m        (used in recurrent policies)\u001B[39;00m\n\u001B[1;32m    538\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 539\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpolicy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobservation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepisode_start\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdeterministic\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/stable_baselines3/common/policies.py:346\u001B[0m, in \u001B[0;36mBasePolicy.predict\u001B[0;34m(self, observation, state, episode_start, deterministic)\u001B[0m\n\u001B[1;32m    343\u001B[0m observation, vectorized_env \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobs_to_tensor(observation)\n\u001B[1;32m    345\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m th\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 346\u001B[0m     actions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_predict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobservation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdeterministic\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdeterministic\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    347\u001B[0m \u001B[38;5;66;03m# Convert to numpy, and reshape to the original action shape\u001B[39;00m\n\u001B[1;32m    348\u001B[0m actions \u001B[38;5;241m=\u001B[39m actions\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy()\u001B[38;5;241m.\u001B[39mreshape((\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_space\u001B[38;5;241m.\u001B[39mshape))\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/stable_baselines3/common/policies.py:676\u001B[0m, in \u001B[0;36mActorCriticPolicy._predict\u001B[0;34m(self, observation, deterministic)\u001B[0m\n\u001B[1;32m    668\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_predict\u001B[39m(\u001B[38;5;28mself\u001B[39m, observation: th\u001B[38;5;241m.\u001B[39mTensor, deterministic: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m th\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[1;32m    669\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    670\u001B[0m \u001B[38;5;124;03m    Get the action according to the policy for a given observation.\u001B[39;00m\n\u001B[1;32m    671\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    674\u001B[0m \u001B[38;5;124;03m    :return: Taken action according to the policy\u001B[39;00m\n\u001B[1;32m    675\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 676\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_distribution\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobservation\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mget_actions(deterministic\u001B[38;5;241m=\u001B[39mdeterministic)\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/stable_baselines3/common/policies.py:711\u001B[0m, in \u001B[0;36mActorCriticPolicy.get_distribution\u001B[0;34m(self, obs)\u001B[0m\n\u001B[1;32m    709\u001B[0m features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mextract_features(obs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpi_features_extractor)\n\u001B[1;32m    710\u001B[0m latent_pi \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmlp_extractor\u001B[38;5;241m.\u001B[39mforward_actor(features)\n\u001B[0;32m--> 711\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_action_dist_from_latent\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlatent_pi\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/stable_baselines3/common/policies.py:659\u001B[0m, in \u001B[0;36mActorCriticPolicy._get_action_dist_from_latent\u001B[0;34m(self, latent_pi)\u001B[0m\n\u001B[1;32m    656\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_dist\u001B[38;5;241m.\u001B[39mproba_distribution(action_logits\u001B[38;5;241m=\u001B[39mmean_actions)\n\u001B[1;32m    657\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_dist, MultiCategoricalDistribution):\n\u001B[1;32m    658\u001B[0m     \u001B[38;5;66;03m# Here mean_actions are the flattened logits\u001B[39;00m\n\u001B[0;32m--> 659\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maction_dist\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mproba_distribution\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction_logits\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmean_actions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    660\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_dist, BernoulliDistribution):\n\u001B[1;32m    661\u001B[0m     \u001B[38;5;66;03m# Here mean_actions are the logits (before rounding to get the binary actions)\u001B[39;00m\n\u001B[1;32m    662\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_dist\u001B[38;5;241m.\u001B[39mproba_distribution(action_logits\u001B[38;5;241m=\u001B[39mmean_actions)\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/stable_baselines3/common/distributions.py:342\u001B[0m, in \u001B[0;36mMultiCategoricalDistribution.proba_distribution\u001B[0;34m(self, action_logits)\u001B[0m\n\u001B[1;32m    339\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mproba_distribution\u001B[39m(\n\u001B[1;32m    340\u001B[0m     \u001B[38;5;28mself\u001B[39m: SelfMultiCategoricalDistribution, action_logits: th\u001B[38;5;241m.\u001B[39mTensor\n\u001B[1;32m    341\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m SelfMultiCategoricalDistribution:\n\u001B[0;32m--> 342\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdistribution \u001B[38;5;241m=\u001B[39m [Categorical(logits\u001B[38;5;241m=\u001B[39msplit) \u001B[38;5;28;01mfor\u001B[39;00m split \u001B[38;5;129;01min\u001B[39;00m th\u001B[38;5;241m.\u001B[39msplit(action_logits, \u001B[38;5;28mtuple\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_dims), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)]\n\u001B[1;32m    343\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/stable_baselines3/common/distributions.py:342\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    339\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mproba_distribution\u001B[39m(\n\u001B[1;32m    340\u001B[0m     \u001B[38;5;28mself\u001B[39m: SelfMultiCategoricalDistribution, action_logits: th\u001B[38;5;241m.\u001B[39mTensor\n\u001B[1;32m    341\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m SelfMultiCategoricalDistribution:\n\u001B[0;32m--> 342\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdistribution \u001B[38;5;241m=\u001B[39m [\u001B[43mCategorical\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlogits\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m split \u001B[38;5;129;01min\u001B[39;00m th\u001B[38;5;241m.\u001B[39msplit(action_logits, \u001B[38;5;28mtuple\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_dims), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)]\n\u001B[1;32m    343\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/distributions/categorical.py:62\u001B[0m, in \u001B[0;36mCategorical.__init__\u001B[0;34m(self, probs, logits, validate_args)\u001B[0m\n\u001B[1;32m     60\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`logits` parameter must be at least one-dimensional.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;66;03m# Normalize\u001B[39;00m\n\u001B[0;32m---> 62\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlogits \u001B[38;5;241m=\u001B[39m logits \u001B[38;5;241m-\u001B[39m \u001B[43mlogits\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlogsumexp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeepdim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_param \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprobs \u001B[38;5;28;01mif\u001B[39;00m probs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlogits\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_events \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_param\u001B[38;5;241m.\u001B[39msize()[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, env, num_episodes=100):\n",
    "    episode_rewards = []\n",
    "    for i in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "        episode_rewards.append(episode_reward)\n",
    "    return np.mean(episode_rewards)\n",
    "\n",
    "# Evaluate the first agent\n",
    "mean_reward1 = evaluate_model(model1, env)\n",
    "print(f\"Mean reward for the first agent: {mean_reward1}\")\n",
    "\n",
    "# Evaluate the second agent\n",
    "mean_reward2 = evaluate_model(model2, env)\n",
    "print(f\"Mean reward for the second agent: {mean_reward2}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
